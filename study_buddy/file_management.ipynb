{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Using Python, you can interact with the OpenAI Assistants API to upload, manage, and check files. Below are the key principles you need to understand:\n",
    "\n",
    "# 1. Upload file to OpenAI\n",
    "\n",
    "To upload a file so that an assistant can access it, use the openai.Files.create() method. The response will include a file_id, which you need to attach to messages.\n",
    "\n",
    "But first you need an openai API key. You can create a key at [platform.openai.com/api-keys](https://platform.openai.com/api-keys). I indend to use my API key in the future, therefore I will hide it using an environment variable. I will also hide any id that I will use."
   ],
   "id": "f81bc3bf9ded9c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-13T22:47:56.782216Z",
     "start_time": "2025-03-13T22:47:56.762024Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "import dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Import environment variables\n",
    "dotenv.load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "# Create session through openai client\n",
    "client = OpenAI(api_key=api_key)"
   ],
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Managing Uploaded Files\n",
    "\n",
    "You may want to manage the files you have uploaded to the model. To check what files have been uploaded use the **list** function. If you want to know the details of a file specifically **pass the file id the retrieve function**. If a file is no longer needed, the **delete** function removes the file with a specific file_id. Remember that multiple instances of the same file can be uploaded."
   ],
   "id": "998369273a392030"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:48:02.819691Z",
     "start_time": "2025-03-13T22:48:01.776749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# What files are accessible to the Assistant\n",
    "files = client.files.list()\n",
    "\n",
    "# Delete all the files with a given name\n",
    "file_name = \"crypto.pdf\"\n",
    "for file in files:\n",
    "    if file.filename == file_name:\n",
    "        client.files.delete(file.id)\n",
    "\n",
    "# Get all files with a given name\n",
    "def get_files_with_filename(file_name):\n",
    "    file_ids = []\n",
    "    files = client.files.list()\n",
    "    for file in files:\n",
    "        if file.filename == file_name:\n",
    "            file_ids.append(file.id)\n",
    "    return file_ids\n",
    "\n",
    "# Information about files with a given name\n",
    "file_name = \"public_perception_of_autonomous_vehicles.pdf\"\n",
    "file_ids = get_files_with_filename(file_name)\n",
    "for id in file_ids:\n",
    "    file_info = client.files.retrieve(id)\n",
    "    print(file_info)\n",
    "\n",
    "del files, file_ids"
   ],
   "id": "37df9c7ea9e0e914",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-9ryu7YKHZR3BcgYzvCd39M', bytes=275218, created_at=1741906032, filename='public_perception_of_autonomous_vehicles.pdf', object='file', purpose='assistants', status='processed', expires_at=None, status_details=None)\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Vector Store\n",
    "\n",
    "To access your files, the *file_search* tool uses the Vector Store object. Upload your files and create a Vector Store to contain them. Once the Vector Store is created, you should poll its status until all files are out of the *in_progress* state to ensure that all content has finished processing. The SDK provides helpers to uploading and polling in one shot."
   ],
   "id": "b39d3a8521d052d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:47:14.978488Z",
     "start_time": "2025-03-13T22:47:10.976980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a vector store to group files\n",
    "vector_store = client.beta.vector_stores.create(name=\"Knowledge Base\")\n",
    "\n",
    "# Ready the files for upload to OpenAI\n",
    "file_paths = [\n",
    "    \"./knowledge_base/cripto.pdf\",\n",
    "    \"./knowledge_base/public_perception_of_autonomous_vehicles.pdf\"\n",
    "]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector\n",
    "# store, and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "# You can print the status and the file counts of the batch\n",
    "# to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ],
   "id": "886be483b263cd75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=2, failed=0, in_progress=0, total=2)\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can also attach files as Message attachments on your thread. Doing so will create another vector_store associated with the thread, or, if there is already a vector store attached to this thread, attach the new files to the existing thread vector store. When you create a Run on this thread, the file search tool will query both the vector_store from your assistant and the vector_store on the thread.",
   "id": "93bd5c0420a0bc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T23:41:38.050421Z",
     "start_time": "2025-03-13T23:41:33.671354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Upload the user provided file to OpenAI\n",
    "message_file = client.files.create(\n",
    "  file=open(\"./knowledge_base/cripto.pdf\", \"rb\"), purpose=\"assistants\"\n",
    ")"
   ],
   "id": "f332a3edd17c21c5",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Create an assistant\n",
    "\n",
    "An assistant in OpenAI’s [Assistants API](https://platform.openai.com/docs/assistants/overview) is a specialized AI entity that can perform tasks, follow instructions, and use tools like code execution and file retrieval. Unlike a general LLM (Large Language Model), which is a standalone model responding to queries based purely on its training data, an assistant is a structured system that includes:\n",
    "\n",
    "\n",
    "- A specific LLM (e.g., GPT-4-turbo) as its foundation.\n",
    "- Custom instructions defining its behavior.\n",
    "- Memory and threads for context retention over multiple interactions.\n",
    "- Tools such as file handling and code execution to enhance functionality.\n",
    "\n",
    "While an LLM is a raw model, an assistant is a wrapper around an LLM, fine-tuned with specific instructions and extended with capabilities such as retrieving uploaded files or running Python code.\n",
    "\n",
    "**Note**: To enable files to be uploaded to an assistant, we first have to provide the *file_search* tool"
   ],
   "id": "8e78863b5f13bf0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:42:42.243266Z",
     "start_time": "2025-03-13T21:42:41.674943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Demo Assistant\",\n",
    "    instructions=\"Process any file that is uploaded and provide insights\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}, {\"type\": \"file_search\"}]\n",
    ")\n",
    "print(\"Assistant ID: \", assistant.id)"
   ],
   "id": "a612e4f00bf48cd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant ID:  asst_2WT9TDtnTH749pRLPkteyLnk\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To make the files accessible to your assistant, update the assistant’s *tool_resources* with the new vector_store id.",
   "id": "226646ef01c9d716"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:56:29.408856Z",
     "start_time": "2025-03-13T22:56:28.255991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ],
   "id": "b14a6f0a8edf3d9",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Assistants API basics\n",
    "\n",
    "Each conversation in the Assistants API happens inside a thread. Threads store the conversation history, allowing for context retention across multiple interactions.\n",
    "To ensure the assistant can access an uploaded file, you need to attach it to a message in a thread.\n",
    "\n",
    "If you want to restart a conversation just create a new thread. Although threads can be deleted explicitly, they are not stored for a long time, and as the last reference to the thread is lost so does the thread. OpenAI handles threads much like garbage manager does unused pointers"
   ],
   "id": "2163520c68d2d631"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:36:25.474006Z",
     "start_time": "2025-03-13T21:36:25.101087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a new thread for messages\n",
    "thread = client.beta.threads.create()\n",
    "thread_id = thread.id\n",
    "print(\"The ID os the new thread is: \", thread_id)"
   ],
   "id": "3b6cea4ed3dcfc2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ID os the new thread is:  thread_Wbrt9hjeCNgDrPQdwzMUVevK\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that you have a thread, send a message inside. Messages represent the user's input to the assistant within the thread. The assistant will respond based on the conversation history.\n",
    "\n",
    "Since the API is asynchronous, you must start a run to make the assistant process the latest message. A run represents the assistant actively working on the request. Runs allow assistants to perform actions like retrieving files, running code, or generating text."
   ],
   "id": "532a8a9d5131c8f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:57:00.785522Z",
     "start_time": "2025-03-13T22:56:58.783706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Add a new message to the thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread_id,\n",
    "    role=\"user\",\n",
    "    content=\"Summarize the files uploaded.\"\n",
    ")\n",
    "\n",
    "# Run the thread to generate a response to the last message\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread_id,\n",
    "    assistant_id=assistant.id,\n",
    "    instructions=\"The user has a deep knowledge about coding,\"\n",
    "                 \"therefore your explanations should account for that\",\n",
    "    attachments=[\n",
    "        { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      ],\n",
    ")\n",
    "# Content sent to the API\n",
    "print(\"The run has the following ID: \", run.id)"
   ],
   "id": "38b2c6978c1de385",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run has the following ID:  run_jeLVsyu8bdjSx2062cA31uga\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Once you send a message to the assistant, it processes the request asynchronously. This means you need to follow a structured approach to retrieve the response. The status of a response can be one of the following:\n",
    "\n",
    "- **in_progress** → Assistant is still processing.\n",
    "- **completed** → Assistant has finished, and a response is available.\n",
    "- **failed** → The run failed due to an error.\n",
    "\n",
    "Once the run is completed, retrieve the assistant’s response from the messages endpoint. The assistant’s response is stored in the thread, so you must fetch the latest messages to see what it generated."
   ],
   "id": "469d24a401738ab3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:57:13.831882Z",
     "start_time": "2025-03-13T22:57:03.923102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    run_status = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread_id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    print(f\"Run Status: {run_status.status}\")\n",
    "\n",
    "    if run_status.status == \"completed\":\n",
    "        messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
    "        for msg in messages.data:\n",
    "            print(f\"{msg.role}: {msg.content[0].text.value}\\n\")\n",
    "        break  # Stop polling once the assistant is done\n",
    "\n",
    "    elif run_status.status == \"failed\":\n",
    "        print(\"Run failed!\")\n",
    "        break\n",
    "    time.sleep(2)  # Wait for 2 seconds before checking again"
   ],
   "id": "d6f1f4198ee43867",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Status: in_progress\n",
      "Run Status: in_progress\n",
      "Run Status: in_progress\n",
      "Run Status: in_progress\n",
      "Run Status: completed\n",
      "assistant: The uploaded file is titled \"Public Perception of Autonomous Vehicles: A Brief Review.\" Here's a summary of its key points:\n",
      "\n",
      "1. **Introduction to Autonomous Vehicles (AVs)**:\n",
      "   - AVs are emerging as a transformative technology in the automotive industry, with the potential to enhance mobility, reduce emissions, and improve safety.\n",
      "   - The success of AVs largely relies on public acceptance, which is influenced by various factors.\n",
      "\n",
      "2. **Public Perception Factors**:\n",
      "   - Several elements shape how different demographics perceive AVs, including age, education, gender, safety concerns, ethical considerations, and impacts from the COVID-19 pandemic.\n",
      "   - Surveys indicate that younger, more educated males tend to have a more favorable view of AVs, whereas fears concerning safety and ethics can lead to negative attitudes.\n",
      "\n",
      "3. **Demographic Insights**:\n",
      "   - Age influences acceptance: younger individuals (especially those aged 25-34) are generally more open to using AVs, while older populations exhibit greater apprehension.\n",
      "   - Gender differences show that men are more inclined to embrace AV technology than women.\n",
      "\n",
      "4. **Safety and Ethical Implications**:\n",
      "   - Public trust in AV safety remains a significant hurdle, compounded by media coverage of accidents involving AVs which can skew public perception negatively.\n",
      "   - Ethical dilemmas arise around decision-making in accident scenarios, reminiscent of moral dilemmas like the trolley problem.\n",
      "\n",
      "5. **COVID-19's Impact**:\n",
      "   - The pandemic has shifted transportation preferences, with a growing interest in AVs as people reconsider public transport's safety post-pandemic.\n",
      "\n",
      "6. **Conclusion and Future Directions**:\n",
      "   - While AVs have advanced significantly since their inception, public skepticism persists, and stakeholders must address concerns around safety and ethics.\n",
      "   - Future research should focus on enhancing public understanding and acceptance, particularly in underrepresented regions and demographics.\n",
      "\n",
      "The authors, Luísa Muglia Souza and José Alberto Barroso Castañon, emphasize the need for continued exploration of the factors influencing public perceptions and the implications for AV policymaking【6:1†source】.\n",
      "\n",
      "user: Summarize the files uploaded.\n",
      "\n",
      "assistant: When you upload files via the OpenAI API, the files are stored temporarily on OpenAI's servers for processing. However, these files are not permanently stored, and any data uploaded is typically deleted after a certain period or after processing is complete, depending on the specific usage policies in place at the time.\n",
      "\n",
      "As for giving access to your assistants or enabling collaboration on these files, you generally have two options:\n",
      "\n",
      "1. **Manual Sharing**: Download the processed data or outputs and manually share them with your assistants via email or file-sharing platforms.\n",
      "\n",
      "2. **API Access**: If you want to programmatically provide access, you could set up your own infrastructure where you manage access and distribution of these files. This could involve allowing your assistants to upload the files to a shared location (like a cloud storage service) where they can also access the files as needed.\n",
      "\n",
      "For exact details on file access or sharing features in OpenAI's API documentation, you should refer to the official OpenAI documentation or any updates pertaining to this functionality.\n",
      "\n",
      "user: Where are uploaded files stored via OpenAI api?How can i give access to my assistants to these files? \n",
      "\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. Assistants File Search\n",
    "\n",
    "File Search augments the Assistant with knowledge from outside its model, such as proprietary product information or documents provided by your users. OpenAI automatically parses and chunks your documents, creates and stores the embeddings, and use both vector and keyword search to retrieve relevant content to answer user queries."
   ],
   "id": "64a9b0d7ccae95ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:02:09.615246Z",
     "start_time": "2025-03-13T21:02:09.354077Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "451b2b8cd9f01aac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FileObject(id='file-F8niMSiNQCguJVAADySRTL', bytes=342004, created_at=1741897816, filename='cripto.pdf', object='file', purpose='assistants', status='processed', expires_at=None, status_details=None), FileObject(id='file-KJV3eTZGm5jVBKjE16kGan', bytes=275218, created_at=1741888415, filename='public_perception_of_autonomous_vehicles.pdf', object='file', purpose='assistants', status='processed', expires_at=None, status_details=None)]\n"
     ]
    }
   ],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
